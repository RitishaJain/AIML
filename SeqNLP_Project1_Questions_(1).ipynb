{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeqNLP_Project1_Questions (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.14"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xT7MKZuMRaCg"
      },
      "source": [
        "# Sentiment Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wq4RCyyPSYRp"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NGCtiXUhSWss",
        "outputId": "ee26597d-f1a5-443b-8afd-236e2daa7d40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "vocab_size = 10000 #vocab size\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size) # vocab_size is no.of words to consider from the dataset, ordering based on frequency."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n",
            "17473536/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCPC_WN-eCyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "vocab_size = 10000 #vocab size\n",
        "maxlen = 300  #number of word used from each review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMEsHYrWxdtk",
        "colab_type": "text"
      },
      "source": [
        "## Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0g381XzeCyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load dataset as a list of ints\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "#make all sequences of the same length\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test =  pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbzzwRJDcc2g",
        "colab_type": "code",
        "outputId": "60090688-8811-43b6-e255-85ab5e93bce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    1,   14,   22,   16,   43,  530,\n",
              "        973, 1622, 1385,   65,  458, 4468,   66, 3941,    4,  173,   36,\n",
              "        256,    5,   25,  100,   43,  838,  112,   50,  670,    2,    9,\n",
              "         35,  480,  284,    5,  150,    4,  172,  112,  167,    2,  336,\n",
              "        385,   39,    4,  172, 4536, 1111,   17,  546,   38,   13,  447,\n",
              "          4,  192,   50,   16,    6,  147, 2025,   19,   14,   22,    4,\n",
              "       1920, 4613,  469,    4,   22,   71,   87,   12,   16,   43,  530,\n",
              "         38,   76,   15,   13, 1247,    4,   22,   17,  515,   17,   12,\n",
              "         16,  626,   18,    2,    5,   62,  386,   12,    8,  316,    8,\n",
              "        106,    5,    4, 2223, 5244,   16,  480,   66, 3785,   33,    4,\n",
              "        130,   12,   16,   38,  619,    5,   25,  124,   51,   36,  135,\n",
              "         48,   25, 1415,   33,    6,   22,   12,  215,   28,   77,   52,\n",
              "          5,   14,  407,   16,   82,    2,    8,    4,  107,  117, 5952,\n",
              "         15,  256,    4,    2,    7, 3766,    5,  723,   36,   71,   43,\n",
              "        530,  476,   26,  400,  317,   46,    7,    4,    2, 1029,   13,\n",
              "        104,   88,    4,  381,   15,  297,   98,   32, 2071,   56,   26,\n",
              "        141,    6,  194, 7486,   18,    4,  226,   22,   21,  134,  476,\n",
              "         26,  480,    5,  144,   30, 5535,   18,   51,   36,   28,  224,\n",
              "         92,   25,  104,    4,  226,   65,   16,   38, 1334,   88,   12,\n",
              "         16,  283,    5,   16, 4472,  113,  103,   32,   15,   16, 5345,\n",
              "         19,  178,   32], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nnrc1njM8YcG"
      },
      "source": [
        "## Create the word index and a key-value pair for word and word_id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZhMAgaNeCy5",
        "colab_type": "code",
        "outputId": "032258e9-b616-4edc-f432-795d4166899f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "word_to_id = imdb.get_word_index()\n",
        "word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
        "word_to_id[\"<PAD>\"] = 0\n",
        "word_to_id[\"<START>\"] = 1\n",
        "word_to_id[\"<UNK>\"] = 2\n",
        "id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "print(' '.join(id_to_word[id] for id in x_train[0] ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n",
            "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dybtUgUReCy8",
        "colab_type": "text"
      },
      "source": [
        "## Build Keras Embedding Layer Model\n",
        "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
        "\n",
        "* The embedding layer can be used at the start of a larger deep learning model. \n",
        "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
        "* Use the embedding layer to train our own word2vec models.\n",
        "\n",
        "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5OLM4eBeCy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxNDNhrseCzA",
        "colab_type": "code",
        "outputId": "c45331a5-53a4-410e-c522-f40b69472b3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "embedding_vector_length = 32 \n",
        "model = Sequential() \n",
        "model.add(Embedding(vocab_size, embedding_vector_length, input_length=maxlen)) \n",
        "model.add(LSTM(100)) \n",
        "model.add(Dense(1, activation='sigmoid')) \n",
        "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy']) \n",
        "print(model.summary()) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 300, 32)           320000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 373,301\n",
            "Trainable params: 373,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERSqY9UHzmHy",
        "colab_type": "code",
        "outputId": "cd6720b2-b891-45f4-c278-8301ccb955ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), nb_epoch=3, batch_size=64) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 116s 5ms/step - loss: 0.4446 - accuracy: 0.7822 - val_loss: 0.3184 - val_accuracy: 0.8684\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 112s 4ms/step - loss: 0.2639 - accuracy: 0.8965 - val_loss: 0.3312 - val_accuracy: 0.8622\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 111s 4ms/step - loss: 0.1991 - accuracy: 0.9270 - val_loss: 0.3201 - val_accuracy: 0.8702\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7efbfa2bbd50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hmW1zNY98w7M"
      },
      "source": [
        "## Accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbVvkfGM99ng",
        "colab_type": "code",
        "outputId": "a4e173bc-57e2-401c-c01e-5926ef49f252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "scores = model.evaluate(x_test, y_test, verbose=0) \n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 87.02%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igq8Qm8GeCzG",
        "colab_type": "text"
      },
      "source": [
        "## Retrive the output of each layer in keras for a given single test sample from the trained model you built"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_4BQSkV_7KT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scYwOWka_PV5",
        "colab_type": "code",
        "outputId": "87cf13fe-b6f0-4112-8604-70250489c013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "outputs = []\n",
        "for layer in model.layers:\n",
        "    keras_function = K.function([model.input], [layer.output])\n",
        "    outputs.append(keras_function([x_test, 1]))\n",
        "print(outputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[array([[[-0.00777039,  0.02623438,  0.01142781, ...,  0.0289028 ,\n",
            "         -0.02578218, -0.00926467],\n",
            "        [-0.00777039,  0.02623438,  0.01142781, ...,  0.0289028 ,\n",
            "         -0.02578218, -0.00926467],\n",
            "        [-0.00777039,  0.02623438,  0.01142781, ...,  0.0289028 ,\n",
            "         -0.02578218, -0.00926467],\n",
            "        ...,\n",
            "        [-0.03498746, -0.04443849, -0.03118311, ..., -0.03166886,\n",
            "          0.00268514,  0.02311075],\n",
            "        [-0.00784201,  0.02104096, -0.00880034, ...,  0.04326359,\n",
            "         -0.04325129, -0.03879954],\n",
            "        [-0.05686688, -0.03734658,  0.00725429, ..., -0.04969543,\n",
            "         -0.01082605, -0.02246371]],\n",
            "\n",
            "       [[-0.00777039,  0.02623438,  0.01142781, ...,  0.0289028 ,\n",
            "         -0.02578218, -0.00926467],\n",
            "        [-0.00777039,  0.02623438,  0.01142781, ...,  0.0289028 ,\n",
            "         -0.02578218, -0.00926467],\n",
            "        [-0.00777039,  0.02623438,  0.01142781, ...,  0.0289028 ,\n",
            "         -0.02578218, -0.00926467],\n",
            "        ...,\n",
            "        [ 0.01601357, -0.04824288, -0.01251326, ..., -0.01991581,\n",
            "          0.08000127,  0.01142533],\n",
            "        [ 0.02996444,  0.0128149 , -0.01790361, ...,  0.04152296,\n",
            "         -0.05508059,  0.00119917],\n",
            "        [-0.01176667,  0.03609512,  0.0361541 , ..., -0.0155033 ,\n",
            "         -0.04419076, -0.02374529]],\n",
            "\n",
            "       [[ 0.00227457,  0.04229312,  0.04449835, ...,  0.01818712,\n",
            "          0.02528947, -0.0179892 ],\n",
            "        [ 0.00971957, -0.01020225,  0.00116985, ...,  0.02554015,\n",
            "         -0.04996752,  0.023139  ],\n",
            "        [-0.0236864 , -0.03533867,  0.01044188, ...,  0.05328945,\n",
            "         -0.03870022,  0.01995281],\n",
            "        ...,\n",
            "        [ 0.02603311,  0.00361809,  0.02357404, ..., -0.02072247,\n",
            "          0.04166111,  0.0106084 ],\n",
            "        [ 0.05320671, -0.01075594, -0.01194803, ..., -0.02568422,\n",
            "          0.06061487, -0.00137389],\n",
            "        [ 0.03122451, -0.11489811,  0.04217074, ..., -0.06689192,\n",
            "          0.06191251, -0.12096698]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-0.00777039,  0.02623438,  0.01142781, ...,  0.0289028 ,\n",
            "         -0.02578218, -0.00926467],\n",
            "        [-0.00777039,  0.02623438,  0.01142781, ...,  0.0289028 ,\n",
            "         -0.02578218, -0.00926467],\n",
            "        [-0.00777039,  0.02623438,  0.01142781, ...,  0.0289028 ,\n",
            "         -0.02578218, -0.00926467],\n",
            "        ...,\n",
            "        [ 0.0110038 , -0.00635104, -0.03234133, ..., -0.00054668,\n",
            "          0.05282507, -0.01737865],\n",
            "        [-0.05035599,  0.03068161, -0.05171071, ...,  0.04643578,\n",
            "          0.01849901,  0.0015496 ],\n",
            "        [ 0.0116152 ,  0.01898956,  0.01422813, ...,  0.00040394,\n",
            "          0.0616437 ,  0.03030551]],\n",
            "\n",
            "       [[-0.00777039,  0.02623438,  0.01142781, ...,  0.0289028 ,\n",
            "         -0.02578218, -0.00926467],\n",
            "        [-0.00777039,  0.02623438,  0.01142781, ...,  0.0289028 ,\n",
            "         -0.02578218, -0.00926467],\n",
            "        [-0.00777039,  0.02623438,  0.01142781, ...,  0.0289028 ,\n",
            "         -0.02578218, -0.00926467],\n",
            "        ...,\n",
            "        [-0.02481873,  0.04440984, -0.04083506, ...,  0.01256806,\n",
            "          0.02930926,  0.04590509],\n",
            "        [-0.04801161, -0.03905587,  0.0284181 , ..., -0.04541944,\n",
            "         -0.03505858, -0.01751656],\n",
            "        [ 0.16743663, -0.1381684 ,  0.19002146, ..., -0.14454795,\n",
            "          0.16942617, -0.10785126]],\n",
            "\n",
            "       [[-0.00777039,  0.02623438,  0.01142781, ...,  0.0289028 ,\n",
            "         -0.02578218, -0.00926467],\n",
            "        [-0.00777039,  0.02623438,  0.01142781, ...,  0.0289028 ,\n",
            "         -0.02578218, -0.00926467],\n",
            "        [-0.00777039,  0.02623438,  0.01142781, ...,  0.0289028 ,\n",
            "         -0.02578218, -0.00926467],\n",
            "        ...,\n",
            "        [ 0.04714935, -0.02357203,  0.01490805, ...,  0.02936923,\n",
            "         -0.00765979, -0.00215941],\n",
            "        [-0.09298738,  0.0651077 , -0.06961613, ...,  0.07811244,\n",
            "         -0.10779449,  0.09362549],\n",
            "        [ 0.06827038, -0.00674358,  0.00951159, ..., -0.02782053,\n",
            "          0.0895612 , -0.04832822]]], dtype=float32)], [array([[ 0.06601644, -0.00444096, -0.05060231, ..., -0.09482063,\n",
            "        -0.0471676 ,  0.05758765],\n",
            "       [-0.1450315 ,  0.00895808,  0.17414802, ...,  0.17239314,\n",
            "         0.16220413, -0.11791757],\n",
            "       [-0.05411144, -0.03214179,  0.04864401, ...,  0.04159359,\n",
            "        -0.00385206,  0.01178952],\n",
            "       ...,\n",
            "       [ 0.09050276, -0.01633701, -0.06886555, ..., -0.13334246,\n",
            "        -0.05374861,  0.09495942],\n",
            "       [ 0.06518558, -0.06015746, -0.06765774, ..., -0.08432313,\n",
            "        -0.0928569 ,  0.06428933],\n",
            "       [-0.03622086,  0.01341592,  0.02489392, ...,  0.05362578,\n",
            "         0.03425954, -0.04661218]], dtype=float32)], [array([[0.10198233],\n",
            "       [0.9847719 ],\n",
            "       [0.73600215],\n",
            "       ...,\n",
            "       [0.05746871],\n",
            "       [0.15313058],\n",
            "       [0.6936433 ]], dtype=float32)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pi3PAkTn9Ouc"
      },
      "source": [
        "## Prediction from the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CNAVINHQ-PQ6"
      },
      "source": [
        "### Prediction from the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3zgS9Y17y13",
        "colab_type": "code",
        "outputId": "273335b5-5b9b-4f67-ecc3-4840c2891974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for review in [x_test[10]]:\n",
        "    tmp = []\n",
        "    tmp_padded = pad_sequences([tmp], maxlen=maxlen) \n",
        "    print(\" Sentiment: %s\" % (model.predict(([tmp_padded][0]))[0][0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Sentiment: 0.2730021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9LRO8DwK9ZIf"
      },
      "source": [
        "#### Here we can see that our model is generating a value for the sentiment.\n",
        "#### If the sentiment value is closer to 0 : The review is classified as a negative review. \n",
        "#### If the sentiment value is closer to 1 : The review is classified as a positive review. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i-1ZBPwK-aI-"
      },
      "source": [
        "### Prediction for custom examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1JvrToi-Bem",
        "colab_type": "code",
        "outputId": "696d7ff5-b644-4f7c-f30c-02e31b471ad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "bad = \"this movie was terrible and bad\"\n",
        "good = \"i really liked the movie and had fun\"\n",
        "for review in [good,bad]:\n",
        "    tmp = []\n",
        "    for word in review.split(\" \"):\n",
        "        tmp.append(word_to_id[word])\n",
        "    tmp_padded = pad_sequences([tmp], maxlen=maxlen) \n",
        "    print(\"%s. Sentiment: %s\" % (review,model.predict(([tmp_padded][0]))[0][0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i really liked the movie and had fun. Sentiment: 0.7557901\n",
            "this movie was terrible and bad. Sentiment: 0.03472362\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dvVc4qLO-pLw"
      },
      "source": [
        "### Here we can see that the positive reviews sentiment value is 0.755 , which is close to 1.\n",
        "### Here we can see that the negative reviews sentiment value is 0.034 , which is close to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yb0ejp6P_DrH"
      },
      "source": [
        "## THANK YOU."
      ]
    }
  ]
}